{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Machine Learning CA 02 Caroline Otten.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.5.4"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iCZYXwtCsL_y"
      },
      "source": [
        "This is a eMail Spam Classifers that uses Naive Bayes supervised machine learning algorithm."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "4p_DvtT7sOIr"
      },
      "source": [
        "#import packages \n",
        "import os\n",
        "import numpy as np\n",
        "from collections import Counter\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.metrics import accuracy_score"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CXknSIrLvzfQ"
      },
      "source": [
        "This function builds a Dictionary of most common 3000 words from all the email content. First it adds all words and symbols in the dictionary. Then it removes all non-alpha-numeric characters and any single character alpha-numeric characters. After this is complete it shrinks the Dictionary by keeping only most common 3000 words in the dictionary. It returns the Dictionary."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "jjKF0nIMwz8_"
      },
      "source": [
        "def make_Dictionary(root_dir):\n",
        "  print(\"Creating dictionary from input\")\n",
        "  all_words = []\n",
        "  emails = [os.path.join(root_dir,f) for f in os.listdir(root_dir)]\n",
        "  print(\"->extracting each word from emails and adding them to a dictionary\")\n",
        "  #Extract each email from all emails\n",
        "  for mail in emails:\n",
        "    with open(mail) as m:\n",
        "      #extract each line from email\n",
        "      for line in m:\n",
        "        #extract each word from email\n",
        "        words = line.split()\n",
        "        #add word list of all words\n",
        "        all_words += words\n",
        "  #convert to a dictionary\n",
        "  dictionary = Counter(all_words)\n",
        "  #create a duplicate as a list\n",
        "  list_to_remove = list(dictionary)\n",
        "\n",
        "  print(\"->removing items from dictionary that include non alphabetical characters or contains more than one word\")\n",
        "  for item in list_to_remove:\n",
        "    #check if item is non alphabetical\n",
        "    if item.isalpha() == False:\n",
        "      del dictionary[item]\n",
        "    #check if item contains more than one word\n",
        "    elif len(item) == 1:\n",
        "      del dictionary[item]\n",
        "  #return dictionary containing the top 3000 most common words in the emails (most_common function comes from collections package) \n",
        "  print(\"-> returning dictionary containing the 3000 most common words in emails\")   \n",
        "  dictionary = dictionary.most_common(3000)\n",
        "  return dictionary\n",
        "            "
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U_NG2-TpxQ1j"
      },
      "source": [
        "This function extracts feature columns and populates their values (Feature Matrix of 3000 comumns and rows equal to the number of email files). The function also analyzes the File Names of each email file and decides if it's a Spam or not based on the naming convention. Based on this the function also creates the Labelled Data Column. This function is used to extract the training dataset as well as the testing dataset and returns the Feature Dataset and the Label column."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "dmVW5xNlyOFc"
      },
      "source": [
        "def extract_features(mail_dir):\n",
        "  print(\"extract email features and put in matrix\")\n",
        "  files = [os.path.join(mail_dir,fi) for fi in os.listdir(mail_dir)]\n",
        "  #Create a matrix(array) filled with zeros that's rows = number of emails and columns = 3000\n",
        "  features_matrix = np.zeros((len(files),3000))\n",
        "  #Create a matrix(array) filled with zeros with 1 row and columns = number of emails\n",
        "  train_labels = np.zeros(len(files))\n",
        "  count = 1;\n",
        "  docID = 0;\n",
        "  #iterate over each email\n",
        "  print(\"->iterating over each email to create matrices containing the words and word counts\")\n",
        "  for fil in files:\n",
        "    with open(fil) as fi:\n",
        "      #extract each line in email - iterate over line in email(line) and track location(i)\n",
        "      for i, line in enumerate(fi):\n",
        "        #split word using white space as separater if i = 2\n",
        "        if i ==2:\n",
        "          words = line.split()\n",
        "          #iterate over words\n",
        "          for word in words:\n",
        "            #set wordID to 0\n",
        "            wordID = 0\n",
        "            #iterate over dictionary and get count each dictionary item - from dictionary function returned dictionary of most common 3000 words in emails and\n",
        "            for i, d in enumerate(dictionary):\n",
        "              #set word id to location of word in dictionary if the first item in dictionary is equal to word in line\n",
        "              if d[0] == word:\n",
        "                wordID = i\n",
        "                #add number of times word appears in line to featrues matrix \n",
        "                features_matrix[docID,wordID] = words.count(word)\n",
        "      train_labels[docID] = 0;\n",
        "      #split email into strings by /\n",
        "      filepathTokens = fil.split('/')\n",
        "      #get the last string\n",
        "      lastToken = filepathTokens[len(filepathTokens)-1]\n",
        "      #if the last string starts with spmsg, change the train_label to 1 instead of 0\n",
        "      if lastToken.startswith(\"spmsg\"):\n",
        "        train_labels[docID] = 1;\n",
        "        #increase count\n",
        "        count = count + 1\n",
        "      #add 1 to doc ID to move down a row in features matrix\n",
        "      docID = docID + 1\n",
        "  return features_matrix, train_labels                "
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AbOV1Y4hxpiy"
      },
      "source": [
        "The section is the main Program that calls the above two functions and gets executed first. First it \"trains\" the model using model.fit function and Training Dataset. After that it scores the Test Data set by running the Trained Model with the Test Data set. At the end it prints the model performance in terms of accuracy score."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ErBHKmBg66tp",
        "outputId": "d242b9b2-1ed5-4877-d272-01fbd7e6fcd5"
      },
      "source": [
        "#get access to goggle drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B-4jgx6hYJYM"
      },
      "source": [
        "#load data\n",
        "TRAIN_DIR = '/content/drive/My Drive/2021/Machine Learning/CA 02/Data (2)/train-mails'\n",
        "TEST_DIR = '/content/drive/My Drive/2021/Machine Learning/CA 02/Data (2)/test-mails'"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "134lmhauyQxE",
        "outputId": "3fc8d5f1-7b92-4b04-95a1-26f9cd49b456"
      },
      "source": [
        "#call make dictionary function to create dictionary of 3000 most common words\n",
        "dictionary = make_Dictionary(TRAIN_DIR)\n",
        "\n",
        "print (\"reading and processing emails from TRAIN and TEST folders\")\n",
        "#generate words and word frequency matrix\n",
        "features_matrix, labels = extract_features(TRAIN_DIR)\n",
        "test_features_matrix, test_labels = extract_features(TEST_DIR)\n",
        "\n",
        "model = GaussianNB()\n",
        "\n",
        "print (\"Training Model using Gaussian Naive Bayes algorithm .....\")\n",
        "model.fit(features_matrix, labels)\n",
        "print (\"Training completed\")\n",
        "print (\"testing trained model to predict Test Data labels\")\n",
        "predicted_labels = model.predict(test_features_matrix)\n",
        "print (\"Completed classification of the Test Data .... now printing Accuracy Score by comparing the Predicted Labels with the Test Labels:\")\n",
        "print (accuracy_score(test_labels, predicted_labels))"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Creating dictionary from input\n",
            "->extracting each word from emails and adding them to a dictionary\n",
            "->removing items from dictionary that include non alphabetical characters or contains more than one word\n",
            "-> returning dictionary containing the 3000 most common words in emails\n",
            "reading and processing emails from TRAIN and TEST folders\n",
            "extract email features and put in matrix\n",
            "->iterating over each email to create matrices containing the words and word counts\n",
            "extract email features and put in matrix\n",
            "->iterating over each email to create matrices containing the words and word counts\n",
            "Training Model using Gaussian Naibe Bayes algorithm .....\n",
            "Training completed\n",
            "testing trained model to predict Test Data labels\n",
            "Completed classification of the Test Data .... now printing Accuracy Score by comparing the Predicted Labels with the Test Labels:\n",
            "0.9615384615384616\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M5_mPrvN586A"
      },
      "source": [
        "======================= END OF PROGRAM ========================="
      ]
    }
  ]
}